# Project Proposal

**Date:** February 6, 2026
**Goal:** Build an anonymous campus rumor verification system without a centralized authority determining truth, relying instead on crowd-driven evidence, Bayesian trust scoring, and anti-manipulation safeguards — demo-ready in one hackathon day.

---

## 1) Problem Statement

Campus rumors spread fast, but truth is hard to validate. We need a system where students can submit rumors anonymously, while other students verify or dispute them without any central authority deciding truth. The system must prevent duplicate voting, resist bots, and avoid popularity-based misinformation. It should also provide an auditable trail of score changes.

### Assumptions

- A per-event or per-semester secret salt is securely distributed once (e.g., via event registration) and is not stored alongside votes.
- Users have access to campus authentication (for initial identity verification) but remain anonymous during voting.

---

## 2) Proposed Solution (MVP Scope)

**Core idea:** Evidence-based challenges + Bayesian trust scoring + bounded popularity, while preserving anonymity via hash-based vote keys.

### Core Features

- Anonymous rumor submission
- Evidence submission (links/images)
- Evidence voting (helpful/misleading)
- Staking layer (users can stake points for higher influence)
- Rumor trust score updates (Bayesian, evidence-weighted)
- Hash-based duplicate vote prevention
- Log-scaled vote impact to cap mob influence
- Append-only audit log for score changes
- Basic bot flags (timing + agreement correlation)

### Stretch / Future Work

- Full trust-graph anomaly detection
- DAG-based dependency propagation for rumor deletions
- Automated evidence credibility scoring
- AI duplicate detection (flag similar rumors to prevent spam)

---

## 3) Functional Requirements (FRs)

**FR-1** Users can submit rumors anonymously.

**FR-2** Users can attach evidence to support or dispute a rumor.

**FR-3** Users can vote on evidence quality (helpful/misleading).

**FR-4** The system computes a rumor trust score using Bayesian updates.

**FR-5** The system prevents duplicate voting per user per rumor via hash-based vote keys.

**FR-6** Rumor scores update in real time after evidence votes.

**FR-7** The system keeps an append-only audit log of all score changes.

**FR-8** The system flags suspicious voting patterns and down-weights them.

**FR-9** Users can view rumor status (Active / Verified / Debunked / Inconclusive).

---

## 4) Non-Functional Requirements (NFRs)

**NFR-1 (Anonymity):** No student identity is stored or linked to votes.

**NFR-2 (Integrity):** All score updates must be reproducible from the audit log.

**NFR-3 (Security):** Hash-based vote keys must be irreversible and unguessable.

**NFR-4 (Fairness):** Popularity alone cannot push a rumor above high trust thresholds.

**NFR-5 (Explainability):** Trust score changes are transparent and auditable.

---

## 5) Architecture Overview (Boilerplate-Agnostic)

- **Frontend:** Provided boilerplate UI framework (to be determined tomorrow)
- **Backend:** Provided boilerplate API layer
- **Database:** Provided storage layer (SQL or NoSQL)
- **Realtime (optional):** If boilerplate supports it, use for live score updates

---

## 6) Data Model (High-Level)

**Entities:**

- `rumors`
- `evidence`
- `evidence_votes`
- `votes`
- `users` (anonymous token hash)
- `audit_log`

---

## 7) AI Moderation (Optional, Low-Risk Add-On)

- **Summarize rumors** for readability
- **Flag harmful content** (warning only, no auto-deletes)

**MVP AI choice:** implement **rumor summarization first** (1–2 lines shown in the feed).

AI never decides truth — only assists the crowd with readability and safety.

---

## 8) Submission Deliverables (Text/Markdown Only)

- Complete project code (via boilerplate fork)
- System design diagrams **in Markdown** (Mermaid or ASCII): ERD, flowchart, architecture
- FYP‑style report (this proposal + approach + summary)
- 5‑slide pitch outline (Markdown bullets)
- Demo‑ready application checklist
- **Note:** AI components are optional enhancements; core functionality remains complete without them.

---

## 9) Edge Cases & Handling

- **Evidence spam:** cap evidence submissions per rumor per user.
- **Duplicate rumors:** AI duplicate detector suggests merge or link to existing rumor.
- **Late evidence:** evidence added after resolution triggers a re-evaluation window.
- **Coordinated voting spikes:** temporary down-weighting and audit log flag.
- **Deleted evidence:** removes downstream influence and recomputes affected scores.

---

## 10) Resistance to Coordinated Manipulation (Proof Sketch)

We model the system as a rational staking game with reputation-weighted Bayesian updates.

- Voting requires staking points.
- Incorrect votes result in irreversible loss of stake.
- Correct votes yield diminishing marginal returns.

Under this model, coordinated false voting requires attackers to incur superlinear cost due to:

1. Logarithmic vote scaling, and
2. Bayesian weighting favoring historically accurate voters.

Therefore, truthful voting is a dominant strategy (Nash Equilibrium), and large-scale manipulation becomes economically irrational.

---

## 11) System Diagrams

### Architecture

```
┌─────────┐
│  User   │
└────┬────┘
     │
     ▼
┌──────────────┐
│  Frontend    │
│  (UI)        │
└──────┬───────┘
       │
       ▼
┌──────────────┐
│  Backend API │
└──────┬───────┘
       │
       ▼
┌──────────────┐
│  Database    │
└──────────────┘
```

### Entity Relationship Diagram

```
RUMORS (1) ──── has ──── (*) EVIDENCE
  │                           │
  │                           │
  │ receives                  │ receives
  │                           │
  (*) VOTES              (*) EVIDENCE_VOTES
  │                           │
  │ casts                     │ casts
  │                           │
USERS ───────────────────── USERS

RUMORS (1) ──── tracked_by ──── (*) AUDIT_LOG
```

### Evidence Voting Flow

```
1. User submits rumor
   User → UI → API → Database (insert rumor)

2. User attaches evidence
   User → UI → API → Database (insert evidence)

3. User votes on evidence
   User → UI → API → Database (insert evidence vote)
                  → Database (update trust score)
                  → Database (log to audit_log)
```
